{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AY8Kx655YU9A",
        "outputId": "efad98f5-fb31-4607-87af-8a7744a93b1c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "!pip install nltk\n",
        "!pip install flask\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import nltk\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import load_model\n",
        "from keras.models import Sequential\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "2XDBJa9DZinB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize Lemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7HfawAfaRgX",
        "outputId": "866102e0-8e6d-41b4-acce-17b2559c2d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#intialize files & Load training data\n",
        "\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = [\"?\", \"!\", \";\", \":\"]\n",
        "data_file = open(\"intents.json\").read()\n",
        "intents = json.loads(data_file)"
      ],
      "metadata": {
        "id": "J8e0Y5hFbylV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize words -> break down a sentence; each word forms a token\n",
        "\n",
        "for intent in intents[\"intents\"]:\n",
        "  for pattern in intent[\"patterns\"]:\n",
        "\n",
        "    w = nltk.word_tokenize(pattern)\n",
        "    words.extend(w)\n",
        "\n",
        "    #add documents\n",
        "    documents.append((w, intent[\"tag\"]))\n",
        "\n",
        "    #add classes to main class list\n",
        "    if intent[\"tag\"] not in classes:\n",
        "      classes.append(intent[\"tag\"])"
      ],
      "metadata": {
        "id": "fmk7R5mUdcZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatize words -> reducing words to their base form\n",
        "\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print(len(documents), \"documents\")\n",
        "print(len(classes), \"classes\", classes)\n",
        "print(len(words), \"unique lemmatized words\", words)\n",
        "\n",
        "pickle.dump(words, open(\"words.pkl\", \"wb\"))\n",
        "pickle.dump(classes, open(\"classes.pkl\", \"wb\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNyw1iRud9UQ",
        "outputId": "f528b4f8-0440-4f9b-8ed4-175984d377ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200 documents\n",
            "55 classes ['AI', 'abbr', 'artificial', 'bend', 'body', 'bot1', 'breathe', 'business', 'chatbot', 'chatterbox', 'clone', 'comp', 'computer', 'control', 'cramped', 'date', 'death', 'do', 'events', 'fav', 'fight', 'goodbye', 'greetings', 'hardware', 'hobby', 'idea', 'imortal', 'lang', 'laugh', 'lie', 'machine', 'malfunction', 'motormouth', 'move', 'name', 'name1', 'need', 'noanswer', 'os', 'program', 'programming', 'ratchet', 'robotics', 'robots', 'robotss', 'sapient', 'sense', 'sentiment', 'shoe', 'sound', 'stupid', 'thanks', 'usage', 'who', 'wt']\n",
            "126 unique lemmatized words [\"'m\", \"'s\", ',', 'a', 'ai', 'all', 'allowed', 'am', 'an', 'are', 'artificial', 'awesome', 'be', 'being', 'bend', 'body', 'bot', 'breathe', 'business', 'bye', 'can', 'chat', 'chatterbox', 'clone', 'coffee', 'computer', 'control', 'cramped', 'data', 'date', 'die', 'do', 'entity', 'event', 'favorite', 'favour', 'fight', 'for', 'good', 'great', 'hardware', 'haroo', 'hello', 'help', 'helpful', 'helping', 'hey', 'hi', 'hobby', 'holla', 'hope', 'how', 'i', 'idea', 'immortal', 'in', 'is', 'it', 'jaw', 'kind', 'language', 'later', 'laugh', 'lie', 'like', 'linguistic', 'making', 'malfunction', 'mate', 'me', 'motormouth', 'move', 'my', 'name', 'need', 'not', 'of', 'okay', 'on', 'operating', 'out', 'over', 'popping', 'product', 'program', 'programming', 'purpose', 'ratchet', 'robot', 'robotics', 'sapient', 'see', 'sense', 'sentient', 'shoe', 'should', 'size', 'sound', 'stupid', 'system', 'take', 'thank', 'thanks', 'thankyou', 'that', 'the', 'there', 'to', 'true', 'type', 'upcoming', 'use', 'walk', 'want', 'wassup', 'what', 'when', 'who', 'will', 'work', 'wow', 'written', 'wtf', 'yaw', 'you', 'your']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a Bag of Words -> train model w/ words to recognize patterns\n",
        "\n",
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "for doc in documents:\n",
        "  #initialise BoW\n",
        "  bag = []\n",
        "\n",
        "  #list of tokenized words for the pattern\n",
        "  pattern_words = doc[0]\n",
        "\n",
        "  #lemmatize each word\n",
        "  pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "\n",
        "  #create BoW -> indicate 1 if word is found in current pattern\n",
        "  for w in words:\n",
        "    bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    #0 for each tag and 1 for current tag (for each pattern)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "  #shuffle features & convert into np.array\n",
        "  random.shuffle(training)\n",
        "\n",
        "  #separate BoW represetations & output labels\n",
        "  train_x = [item[0] for item in training]\n",
        "  train_y = [item[1] for item in training]\n",
        "\n",
        "  #convert to NumPy arrays\n",
        "  train_x = np.array(train_x)\n",
        "  train_y = np.array(train_y)\n",
        "  print(\"Training data created\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3EleXIw0evwK",
        "outputId": "dd7d1910-7f9d-4ca5-8a5b-900dc1b53038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n",
            "Training data created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a model w/ 3 layers;\n",
        "#layer 1 - 128 neurons\n",
        "#layer 2 - 64 neurons\n",
        "#layer 3 - no. of neurons = no. of intents to predict output intent w/ softmax\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation=\"softmax\"))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKYRZyprgWEv",
        "outputId": "979f883b-4024-4d4b-a124-cb169a04c35b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               16256     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 55)                3575      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 28087 (109.71 KB)\n",
            "Trainable params: 28087 (109.71 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compile model\n",
        "\n",
        "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "qVtSGE8fhQn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fitting & saving the model\n",
        "\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
        "model.save(\"chatbot_model_v3.h5\", hist)\n",
        "print(\"Model created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL0ammyHh9oI",
        "outputId": "69e247e1-4506-4b93-e93f-0c11de9d2223",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "6318/6318 [==============================] - 32s 5ms/step - loss: 0.0719 - accuracy: 0.9841\n",
            "Epoch 2/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 0.0798 - accuracy: 0.9817\n",
            "Epoch 3/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 0.0726 - accuracy: 0.9846\n",
            "Epoch 4/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 0.1133 - accuracy: 0.9779\n",
            "Epoch 5/200\n",
            "6318/6318 [==============================] - 26s 4ms/step - loss: 0.1614 - accuracy: 0.9716\n",
            "Epoch 6/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 0.2156 - accuracy: 0.9663\n",
            "Epoch 7/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 0.4993 - accuracy: 0.9337\n",
            "Epoch 8/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 0.8628 - accuracy: 0.8988\n",
            "Epoch 9/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.0743 - accuracy: 0.8247\n",
            "Epoch 10/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 0.7630 - accuracy: 0.8334\n",
            "Epoch 11/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 0.7454 - accuracy: 0.8289\n",
            "Epoch 12/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 0.7179 - accuracy: 0.8133\n",
            "Epoch 13/200\n",
            "6318/6318 [==============================] - 24s 4ms/step - loss: 0.6952 - accuracy: 0.8053\n",
            "Epoch 14/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 0.6814 - accuracy: 0.8078\n",
            "Epoch 15/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 0.6889 - accuracy: 0.8051\n",
            "Epoch 16/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 0.6981 - accuracy: 0.7977\n",
            "Epoch 17/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 0.6968 - accuracy: 0.7948\n",
            "Epoch 18/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 0.7339 - accuracy: 0.7841\n",
            "Epoch 19/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 0.7334 - accuracy: 0.7795\n",
            "Epoch 20/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 0.7712 - accuracy: 0.7667\n",
            "Epoch 21/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 0.8347 - accuracy: 0.7584\n",
            "Epoch 22/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 0.8261 - accuracy: 0.7565\n",
            "Epoch 23/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 0.8466 - accuracy: 0.7549\n",
            "Epoch 24/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 0.8318 - accuracy: 0.7535\n",
            "Epoch 25/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 0.8389 - accuracy: 0.7535\n",
            "Epoch 26/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 0.8841 - accuracy: 0.7405\n",
            "Epoch 27/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 0.8905 - accuracy: 0.7385\n",
            "Epoch 28/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 0.8969 - accuracy: 0.7348\n",
            "Epoch 29/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 0.9023 - accuracy: 0.7371\n",
            "Epoch 30/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 0.9024 - accuracy: 0.7381\n",
            "Epoch 31/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 0.8889 - accuracy: 0.7382\n",
            "Epoch 32/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 0.8984 - accuracy: 0.7357\n",
            "Epoch 33/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 0.8906 - accuracy: 0.7386\n",
            "Epoch 34/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 0.9107 - accuracy: 0.7357\n",
            "Epoch 35/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 0.9483 - accuracy: 0.7264\n",
            "Epoch 36/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 0.9504 - accuracy: 0.7213\n",
            "Epoch 37/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 0.9593 - accuracy: 0.7209\n",
            "Epoch 38/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 0.9859 - accuracy: 0.7115\n",
            "Epoch 39/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 0.9799 - accuracy: 0.7143\n",
            "Epoch 40/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.0111 - accuracy: 0.7107\n",
            "Epoch 41/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 0.9976 - accuracy: 0.7107\n",
            "Epoch 42/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 0.9972 - accuracy: 0.7097\n",
            "Epoch 43/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.0197 - accuracy: 0.7016\n",
            "Epoch 44/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.0182 - accuracy: 0.7033\n",
            "Epoch 45/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.0325 - accuracy: 0.7008\n",
            "Epoch 46/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.0328 - accuracy: 0.6998\n",
            "Epoch 47/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 1.0306 - accuracy: 0.6995\n",
            "Epoch 48/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.0397 - accuracy: 0.6982\n",
            "Epoch 49/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.0318 - accuracy: 0.6991\n",
            "Epoch 50/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 1.0508 - accuracy: 0.6952\n",
            "Epoch 51/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.0707 - accuracy: 0.6886\n",
            "Epoch 52/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.0724 - accuracy: 0.6890\n",
            "Epoch 53/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 1.0708 - accuracy: 0.6885\n",
            "Epoch 54/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.0641 - accuracy: 0.6902\n",
            "Epoch 55/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.1055 - accuracy: 0.6801\n",
            "Epoch 56/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.1206 - accuracy: 0.6728\n",
            "Epoch 57/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.1429 - accuracy: 0.6643\n",
            "Epoch 58/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.1349 - accuracy: 0.6647\n",
            "Epoch 59/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.1276 - accuracy: 0.6660\n",
            "Epoch 60/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 1.1204 - accuracy: 0.6673\n",
            "Epoch 61/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.1283 - accuracy: 0.6667\n",
            "Epoch 62/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.1395 - accuracy: 0.6670\n",
            "Epoch 63/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.1246 - accuracy: 0.6697\n",
            "Epoch 64/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.1350 - accuracy: 0.6686\n",
            "Epoch 65/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.1431 - accuracy: 0.6643\n",
            "Epoch 66/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.1465 - accuracy: 0.6633\n",
            "Epoch 67/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.1464 - accuracy: 0.6650\n",
            "Epoch 68/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.1766 - accuracy: 0.6585\n",
            "Epoch 69/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.1707 - accuracy: 0.6583\n",
            "Epoch 70/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.1684 - accuracy: 0.6592\n",
            "Epoch 71/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.1689 - accuracy: 0.6589\n",
            "Epoch 72/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.1780 - accuracy: 0.6564\n",
            "Epoch 73/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.1875 - accuracy: 0.6541\n",
            "Epoch 74/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.1689 - accuracy: 0.6590\n",
            "Epoch 75/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.1791 - accuracy: 0.6556\n",
            "Epoch 76/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.1948 - accuracy: 0.6538\n",
            "Epoch 77/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.1747 - accuracy: 0.6572\n",
            "Epoch 78/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.2015 - accuracy: 0.6506\n",
            "Epoch 79/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 1.1891 - accuracy: 0.6528\n",
            "Epoch 80/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.2183 - accuracy: 0.6441\n",
            "Epoch 81/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.2195 - accuracy: 0.6449\n",
            "Epoch 82/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.2285 - accuracy: 0.6419\n",
            "Epoch 83/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.2204 - accuracy: 0.6437\n",
            "Epoch 84/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.2817 - accuracy: 0.6235\n",
            "Epoch 85/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.2649 - accuracy: 0.6255\n",
            "Epoch 86/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.2816 - accuracy: 0.6214\n",
            "Epoch 87/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.2695 - accuracy: 0.6260\n",
            "Epoch 88/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.2506 - accuracy: 0.6321\n",
            "Epoch 89/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 1.2656 - accuracy: 0.6259\n",
            "Epoch 90/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.2470 - accuracy: 0.6317\n",
            "Epoch 91/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.2730 - accuracy: 0.6255\n",
            "Epoch 92/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 1.2681 - accuracy: 0.6270\n",
            "Epoch 93/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.2681 - accuracy: 0.6268\n",
            "Epoch 94/200\n",
            "6318/6318 [==============================] - 24s 4ms/step - loss: 1.2757 - accuracy: 0.6236\n",
            "Epoch 95/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.2563 - accuracy: 0.6293\n",
            "Epoch 96/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.2651 - accuracy: 0.6265\n",
            "Epoch 97/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.2738 - accuracy: 0.6261\n",
            "Epoch 98/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.3037 - accuracy: 0.6179\n",
            "Epoch 99/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.3041 - accuracy: 0.6159\n",
            "Epoch 100/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.3078 - accuracy: 0.6166\n",
            "Epoch 101/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.3043 - accuracy: 0.6160\n",
            "Epoch 102/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 1.2872 - accuracy: 0.6200\n",
            "Epoch 103/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.2965 - accuracy: 0.6171\n",
            "Epoch 104/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.3488 - accuracy: 0.6012\n",
            "Epoch 105/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 1.3718 - accuracy: 0.5927\n",
            "Epoch 106/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.3756 - accuracy: 0.5935\n",
            "Epoch 107/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.3897 - accuracy: 0.5894\n",
            "Epoch 108/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 1.3704 - accuracy: 0.5940\n",
            "Epoch 109/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.4008 - accuracy: 0.5880\n",
            "Epoch 110/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.3890 - accuracy: 0.5895\n",
            "Epoch 111/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.3984 - accuracy: 0.5855\n",
            "Epoch 112/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.4324 - accuracy: 0.5738\n",
            "Epoch 113/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.4308 - accuracy: 0.5772\n",
            "Epoch 114/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.4247 - accuracy: 0.5771\n",
            "Epoch 115/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.4281 - accuracy: 0.5746\n",
            "Epoch 116/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.4247 - accuracy: 0.5773\n",
            "Epoch 117/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.4278 - accuracy: 0.5750\n",
            "Epoch 118/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 1.4362 - accuracy: 0.5725\n",
            "Epoch 119/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.4635 - accuracy: 0.5629\n",
            "Epoch 120/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.4490 - accuracy: 0.5655\n",
            "Epoch 121/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.4584 - accuracy: 0.5620\n",
            "Epoch 122/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.4745 - accuracy: 0.5596\n",
            "Epoch 123/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.4513 - accuracy: 0.5659\n",
            "Epoch 124/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.4582 - accuracy: 0.5622\n",
            "Epoch 125/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.4686 - accuracy: 0.5611\n",
            "Epoch 126/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.4731 - accuracy: 0.5620\n",
            "Epoch 127/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.4800 - accuracy: 0.5597\n",
            "Epoch 128/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.4796 - accuracy: 0.5604\n",
            "Epoch 129/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.4735 - accuracy: 0.5625\n",
            "Epoch 130/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.4769 - accuracy: 0.5614\n",
            "Epoch 131/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.4687 - accuracy: 0.5633\n",
            "Epoch 132/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 1.4638 - accuracy: 0.5634\n",
            "Epoch 133/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.4745 - accuracy: 0.5610\n",
            "Epoch 134/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.4822 - accuracy: 0.5596\n",
            "Epoch 135/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 1.4661 - accuracy: 0.5614\n",
            "Epoch 136/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.4965 - accuracy: 0.5535\n",
            "Epoch 137/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.5078 - accuracy: 0.5522\n",
            "Epoch 138/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 1.4974 - accuracy: 0.5552\n",
            "Epoch 139/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.4964 - accuracy: 0.5567\n",
            "Epoch 140/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.4968 - accuracy: 0.5572\n",
            "Epoch 141/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 1.5215 - accuracy: 0.5504\n",
            "Epoch 142/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.5099 - accuracy: 0.5526\n",
            "Epoch 143/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.5332 - accuracy: 0.5455\n",
            "Epoch 144/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 1.5334 - accuracy: 0.5445\n",
            "Epoch 145/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.5282 - accuracy: 0.5459\n",
            "Epoch 146/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.5264 - accuracy: 0.5462\n",
            "Epoch 147/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 1.5313 - accuracy: 0.5460\n",
            "Epoch 148/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.5432 - accuracy: 0.5426\n",
            "Epoch 149/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.5592 - accuracy: 0.5368\n",
            "Epoch 150/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 1.5457 - accuracy: 0.5407\n",
            "Epoch 151/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.5424 - accuracy: 0.5412\n",
            "Epoch 152/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.5401 - accuracy: 0.5420\n",
            "Epoch 153/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.5371 - accuracy: 0.5427\n",
            "Epoch 154/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.5421 - accuracy: 0.5422\n",
            "Epoch 155/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.5278 - accuracy: 0.5446\n",
            "Epoch 156/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 1.5395 - accuracy: 0.5434\n",
            "Epoch 157/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.5484 - accuracy: 0.5405\n",
            "Epoch 158/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.5276 - accuracy: 0.5466\n",
            "Epoch 159/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.5172 - accuracy: 0.5492\n",
            "Epoch 160/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.5440 - accuracy: 0.5438\n",
            "Epoch 161/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.5679 - accuracy: 0.5378\n",
            "Epoch 162/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.5534 - accuracy: 0.5387\n",
            "Epoch 163/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.5595 - accuracy: 0.5376\n",
            "Epoch 164/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.5465 - accuracy: 0.5401\n",
            "Epoch 165/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.5530 - accuracy: 0.5400\n",
            "Epoch 166/200\n",
            "6318/6318 [==============================] - 21s 3ms/step - loss: 1.5628 - accuracy: 0.5365\n",
            "Epoch 167/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.5469 - accuracy: 0.5400\n",
            "Epoch 168/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.5751 - accuracy: 0.5339\n",
            "Epoch 169/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.5645 - accuracy: 0.5373\n",
            "Epoch 170/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.5806 - accuracy: 0.5316\n",
            "Epoch 171/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.5741 - accuracy: 0.5338\n",
            "Epoch 172/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.5745 - accuracy: 0.5342\n",
            "Epoch 173/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.5763 - accuracy: 0.5337\n",
            "Epoch 174/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.5683 - accuracy: 0.5343\n",
            "Epoch 175/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.5698 - accuracy: 0.5361\n",
            "Epoch 176/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.5622 - accuracy: 0.5373\n",
            "Epoch 177/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.5683 - accuracy: 0.5366\n",
            "Epoch 178/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.5730 - accuracy: 0.5332\n",
            "Epoch 179/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.5763 - accuracy: 0.5337\n",
            "Epoch 180/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.5661 - accuracy: 0.5366\n",
            "Epoch 181/200\n",
            "6318/6318 [==============================] - 24s 4ms/step - loss: 1.5543 - accuracy: 0.5400\n",
            "Epoch 182/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.5605 - accuracy: 0.5371\n",
            "Epoch 183/200\n",
            "6318/6318 [==============================] - 22s 3ms/step - loss: 1.5460 - accuracy: 0.5406\n",
            "Epoch 184/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.5543 - accuracy: 0.5397\n",
            "Epoch 185/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.5710 - accuracy: 0.5343\n",
            "Epoch 186/200\n",
            "6318/6318 [==============================] - 23s 4ms/step - loss: 1.5677 - accuracy: 0.5357\n",
            "Epoch 187/200\n",
            "6318/6318 [==============================] - 22s 4ms/step - loss: 1.5732 - accuracy: 0.5345\n",
            "Epoch 188/200\n",
            "6318/6318 [==============================] - 29s 5ms/step - loss: 1.5564 - accuracy: 0.5387\n",
            "Epoch 189/200\n",
            "6318/6318 [==============================] - 31s 5ms/step - loss: 1.5478 - accuracy: 0.5413\n",
            "Epoch 190/200\n",
            "6318/6318 [==============================] - 31s 5ms/step - loss: 1.5869 - accuracy: 0.5315\n",
            "Epoch 191/200\n",
            "6318/6318 [==============================] - 31s 5ms/step - loss: 1.5760 - accuracy: 0.5317\n",
            "Epoch 192/200\n",
            "6318/6318 [==============================] - 30s 5ms/step - loss: 1.5912 - accuracy: 0.5286\n",
            "Epoch 193/200\n",
            "6318/6318 [==============================] - 30s 5ms/step - loss: 1.5949 - accuracy: 0.5286\n",
            "Epoch 194/200\n",
            "6318/6318 [==============================] - 30s 5ms/step - loss: 1.5843 - accuracy: 0.5309\n",
            "Epoch 195/200\n",
            "6318/6318 [==============================] - 30s 5ms/step - loss: 1.5902 - accuracy: 0.5292\n",
            "Epoch 196/200\n",
            "6318/6318 [==============================] - 30s 5ms/step - loss: 1.5974 - accuracy: 0.5279\n",
            "Epoch 197/200\n",
            "6318/6318 [==============================] - 30s 5ms/step - loss: 1.5966 - accuracy: 0.5277\n",
            "Epoch 198/200\n",
            "6318/6318 [==============================] - 30s 5ms/step - loss: 1.5987 - accuracy: 0.5272\n",
            "Epoch 199/200\n",
            "6318/6318 [==============================] - 30s 5ms/step - loss: 1.5881 - accuracy: 0.5296\n",
            "Epoch 200/200\n",
            "6318/6318 [==============================] - 29s 5ms/step - loss: 1.5911 - accuracy: 0.5287\n",
            "Model created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgQdC0nFh8-t",
        "outputId": "2181111a-e142-4c4a-ad85-9e569223428b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kzCWBazqhDy4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa47956f-81d0-49e6-e8bf-69fe5ba5db34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "vFd3msIMib0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#fitting & saving the model ->> round 2\n",
        "\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
        "model.save('/content/drive/My Drive/ColabModels/chatbot_v3.h5', hist)\n",
        "print(\"Model created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "S8yZ9ibTldDc",
        "outputId": "e7b111bf-813a-46fe-932e-339e3dc4bdb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch 1/200\n",
            "5040/5040 [==============================] - 20s 3ms/step - loss: 0.7701 - accuracy: 0.7854\n",
            "Epoch 2/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 0.2129 - accuracy: 0.9286\n",
            "Epoch 3/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 0.1624 - accuracy: 0.9433\n",
            "Epoch 4/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 0.1593 - accuracy: 0.9465\n",
            "Epoch 5/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 0.1633 - accuracy: 0.9468\n",
            "Epoch 6/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 0.1807 - accuracy: 0.9419\n",
            "Epoch 7/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 0.2014 - accuracy: 0.9392\n",
            "Epoch 8/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 0.2089 - accuracy: 0.9385\n",
            "Epoch 9/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 0.2471 - accuracy: 0.9328\n",
            "Epoch 10/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 0.2739 - accuracy: 0.9295\n",
            "Epoch 11/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 0.3071 - accuracy: 0.9263\n",
            "Epoch 12/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 0.3980 - accuracy: 0.9150\n",
            "Epoch 13/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 0.4306 - accuracy: 0.9081\n",
            "Epoch 14/200\n",
            "5040/5040 [==============================] - 18s 3ms/step - loss: 0.5702 - accuracy: 0.8912\n",
            "Epoch 15/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 0.6286 - accuracy: 0.8821\n",
            "Epoch 16/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 0.5979 - accuracy: 0.8846\n",
            "Epoch 17/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 0.6121 - accuracy: 0.8823\n",
            "Epoch 18/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 0.6289 - accuracy: 0.8787\n",
            "Epoch 19/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 0.6636 - accuracy: 0.8662\n",
            "Epoch 20/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 0.6953 - accuracy: 0.8591\n",
            "Epoch 21/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 0.6005 - accuracy: 0.8677\n",
            "Epoch 22/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 0.6382 - accuracy: 0.8633\n",
            "Epoch 23/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 0.7229 - accuracy: 0.8485\n",
            "Epoch 24/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 0.7864 - accuracy: 0.8343\n",
            "Epoch 25/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 0.8086 - accuracy: 0.8293\n",
            "Epoch 26/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 0.8232 - accuracy: 0.8218\n",
            "Epoch 27/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 0.7854 - accuracy: 0.8215\n",
            "Epoch 28/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 0.8908 - accuracy: 0.8021\n",
            "Epoch 29/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 0.8485 - accuracy: 0.7947\n",
            "Epoch 30/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 0.8792 - accuracy: 0.7918\n",
            "Epoch 31/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 0.9309 - accuracy: 0.7797\n",
            "Epoch 32/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 0.9837 - accuracy: 0.7622\n",
            "Epoch 33/200\n",
            "5040/5040 [==============================] - 15s 3ms/step - loss: 1.0241 - accuracy: 0.7479\n",
            "Epoch 34/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.0018 - accuracy: 0.7501\n",
            "Epoch 35/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.0388 - accuracy: 0.7444\n",
            "Epoch 36/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.0261 - accuracy: 0.7398\n",
            "Epoch 37/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.1484 - accuracy: 0.7184\n",
            "Epoch 38/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.1187 - accuracy: 0.7242\n",
            "Epoch 39/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.1673 - accuracy: 0.7108\n",
            "Epoch 40/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.1878 - accuracy: 0.7068\n",
            "Epoch 41/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.1869 - accuracy: 0.7007\n",
            "Epoch 42/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.1771 - accuracy: 0.7027\n",
            "Epoch 43/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.1641 - accuracy: 0.7069\n",
            "Epoch 44/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.1798 - accuracy: 0.7065\n",
            "Epoch 45/200\n",
            "5040/5040 [==============================] - 18s 3ms/step - loss: 1.2030 - accuracy: 0.6985\n",
            "Epoch 46/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.1775 - accuracy: 0.7020\n",
            "Epoch 47/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.2194 - accuracy: 0.6912\n",
            "Epoch 48/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.2337 - accuracy: 0.6897\n",
            "Epoch 49/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.2502 - accuracy: 0.6852\n",
            "Epoch 50/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.3045 - accuracy: 0.6749\n",
            "Epoch 51/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.3348 - accuracy: 0.6684\n",
            "Epoch 52/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.3499 - accuracy: 0.6706\n",
            "Epoch 53/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.3391 - accuracy: 0.6699\n",
            "Epoch 54/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.3711 - accuracy: 0.6644\n",
            "Epoch 55/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.3873 - accuracy: 0.6628\n",
            "Epoch 56/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.3651 - accuracy: 0.6625\n",
            "Epoch 57/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.3673 - accuracy: 0.6599\n",
            "Epoch 58/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.3632 - accuracy: 0.6609\n",
            "Epoch 59/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.3698 - accuracy: 0.6607\n",
            "Epoch 60/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 1.3827 - accuracy: 0.6561\n",
            "Epoch 61/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.4064 - accuracy: 0.6518\n",
            "Epoch 62/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.4070 - accuracy: 0.6505\n",
            "Epoch 63/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.3966 - accuracy: 0.6534\n",
            "Epoch 64/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.3938 - accuracy: 0.6531\n",
            "Epoch 65/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.4176 - accuracy: 0.6494\n",
            "Epoch 66/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.4000 - accuracy: 0.6519\n",
            "Epoch 67/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.4030 - accuracy: 0.6512\n",
            "Epoch 68/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.4139 - accuracy: 0.6522\n",
            "Epoch 69/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.4241 - accuracy: 0.6488\n",
            "Epoch 70/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.4581 - accuracy: 0.6446\n",
            "Epoch 71/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.4682 - accuracy: 0.6449\n",
            "Epoch 72/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.4695 - accuracy: 0.6435\n",
            "Epoch 73/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.4605 - accuracy: 0.6452\n",
            "Epoch 74/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.4729 - accuracy: 0.6434\n",
            "Epoch 75/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.4937 - accuracy: 0.6376\n",
            "Epoch 76/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.4775 - accuracy: 0.6413\n",
            "Epoch 77/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.4887 - accuracy: 0.6394\n",
            "Epoch 78/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.4776 - accuracy: 0.6414\n",
            "Epoch 79/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.4968 - accuracy: 0.6393\n",
            "Epoch 80/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.5386 - accuracy: 0.6318\n",
            "Epoch 81/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.5532 - accuracy: 0.6322\n",
            "Epoch 82/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.5295 - accuracy: 0.6319\n",
            "Epoch 83/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.5462 - accuracy: 0.6309\n",
            "Epoch 84/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.5450 - accuracy: 0.6277\n",
            "Epoch 85/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.5580 - accuracy: 0.6252\n",
            "Epoch 86/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.5505 - accuracy: 0.6279\n",
            "Epoch 87/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.5476 - accuracy: 0.6316\n",
            "Epoch 88/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.5418 - accuracy: 0.6292\n",
            "Epoch 89/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.5543 - accuracy: 0.6279\n",
            "Epoch 90/200\n",
            "5040/5040 [==============================] - 19s 4ms/step - loss: 1.5911 - accuracy: 0.6201\n",
            "Epoch 91/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.5940 - accuracy: 0.6187\n",
            "Epoch 92/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6183 - accuracy: 0.6184\n",
            "Epoch 93/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6481 - accuracy: 0.6187\n",
            "Epoch 94/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6590 - accuracy: 0.6180\n",
            "Epoch 95/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 1.6698 - accuracy: 0.6163\n",
            "Epoch 96/200\n",
            "5040/5040 [==============================] - 18s 3ms/step - loss: 1.6587 - accuracy: 0.6173\n",
            "Epoch 97/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6690 - accuracy: 0.6149\n",
            "Epoch 98/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6622 - accuracy: 0.6180\n",
            "Epoch 99/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6559 - accuracy: 0.6183\n",
            "Epoch 100/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6775 - accuracy: 0.6142\n",
            "Epoch 101/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6481 - accuracy: 0.6168\n",
            "Epoch 102/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6359 - accuracy: 0.6179\n",
            "Epoch 103/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6259 - accuracy: 0.6196\n",
            "Epoch 104/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 1.6582 - accuracy: 0.6137\n",
            "Epoch 105/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6381 - accuracy: 0.6163\n",
            "Epoch 106/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6456 - accuracy: 0.6150\n",
            "Epoch 107/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6638 - accuracy: 0.6136\n",
            "Epoch 108/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6555 - accuracy: 0.6144\n",
            "Epoch 109/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6601 - accuracy: 0.6140\n",
            "Epoch 110/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6539 - accuracy: 0.6150\n",
            "Epoch 111/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 1.6508 - accuracy: 0.6160\n",
            "Epoch 112/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6569 - accuracy: 0.6185\n",
            "Epoch 113/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6761 - accuracy: 0.6137\n",
            "Epoch 114/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6733 - accuracy: 0.6149\n",
            "Epoch 115/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6790 - accuracy: 0.6148\n",
            "Epoch 116/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6672 - accuracy: 0.6158\n",
            "Epoch 117/200\n",
            "5040/5040 [==============================] - 19s 4ms/step - loss: 1.6693 - accuracy: 0.6153\n",
            "Epoch 118/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6915 - accuracy: 0.6108\n",
            "Epoch 119/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.7002 - accuracy: 0.6090\n",
            "Epoch 120/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.6979 - accuracy: 0.6094\n",
            "Epoch 121/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.7011 - accuracy: 0.6080\n",
            "Epoch 122/200\n",
            "5040/5040 [==============================] - 18s 3ms/step - loss: 1.6988 - accuracy: 0.6086\n",
            "Epoch 123/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 1.7079 - accuracy: 0.6083\n",
            "Epoch 124/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.7111 - accuracy: 0.6068\n",
            "Epoch 125/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.7253 - accuracy: 0.6075\n",
            "Epoch 126/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.7237 - accuracy: 0.6044\n",
            "Epoch 127/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.7073 - accuracy: 0.6066\n",
            "Epoch 128/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 1.7139 - accuracy: 0.6051\n",
            "Epoch 129/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.7244 - accuracy: 0.6033\n",
            "Epoch 130/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.7249 - accuracy: 0.6040\n",
            "Epoch 131/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 1.7228 - accuracy: 0.6029\n",
            "Epoch 132/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.7021 - accuracy: 0.6087\n",
            "Epoch 133/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 1.7178 - accuracy: 0.6059\n",
            "Epoch 134/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.7056 - accuracy: 0.6079\n",
            "Epoch 135/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.7059 - accuracy: 0.6077\n",
            "Epoch 136/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.7010 - accuracy: 0.6095\n",
            "Epoch 137/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.7354 - accuracy: 0.6023\n",
            "Epoch 138/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 1.7410 - accuracy: 0.5996\n",
            "Epoch 139/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.7241 - accuracy: 0.6042\n",
            "Epoch 140/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.7251 - accuracy: 0.6029\n",
            "Epoch 141/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.7204 - accuracy: 0.6048\n",
            "Epoch 142/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.7407 - accuracy: 0.6039\n",
            "Epoch 143/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 1.8006 - accuracy: 0.5861\n",
            "Epoch 144/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 1.7876 - accuracy: 0.5868\n",
            "Epoch 145/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.7883 - accuracy: 0.5863\n",
            "Epoch 146/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.7910 - accuracy: 0.5865\n",
            "Epoch 147/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.7961 - accuracy: 0.5847\n",
            "Epoch 148/200\n",
            "5040/5040 [==============================] - 18s 3ms/step - loss: 1.8116 - accuracy: 0.5818\n",
            "Epoch 149/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.8061 - accuracy: 0.5834\n",
            "Epoch 150/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.8079 - accuracy: 0.5837\n",
            "Epoch 151/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.8114 - accuracy: 0.5820\n",
            "Epoch 152/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.8136 - accuracy: 0.5811\n",
            "Epoch 153/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.8197 - accuracy: 0.5813\n",
            "Epoch 154/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.8400 - accuracy: 0.5759\n",
            "Epoch 155/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.8335 - accuracy: 0.5764\n",
            "Epoch 156/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.8346 - accuracy: 0.5761\n",
            "Epoch 157/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 1.8353 - accuracy: 0.5767\n",
            "Epoch 158/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 1.8567 - accuracy: 0.5719\n",
            "Epoch 159/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.8719 - accuracy: 0.5693\n",
            "Epoch 160/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.8508 - accuracy: 0.5727\n",
            "Epoch 161/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.8530 - accuracy: 0.5729\n",
            "Epoch 162/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.8467 - accuracy: 0.5735\n",
            "Epoch 163/200\n",
            "5040/5040 [==============================] - 19s 4ms/step - loss: 1.8875 - accuracy: 0.5682\n",
            "Epoch 164/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 1.8843 - accuracy: 0.5650\n",
            "Epoch 165/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.9057 - accuracy: 0.5579\n",
            "Epoch 166/200\n",
            "5040/5040 [==============================] - 18s 3ms/step - loss: 1.8856 - accuracy: 0.5644\n",
            "Epoch 167/200\n",
            "5040/5040 [==============================] - 19s 4ms/step - loss: 1.8559 - accuracy: 0.5718\n",
            "Epoch 168/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 1.8576 - accuracy: 0.5710\n",
            "Epoch 169/200\n",
            "5040/5040 [==============================] - 18s 3ms/step - loss: 1.8717 - accuracy: 0.5676\n",
            "Epoch 170/200\n",
            "5040/5040 [==============================] - 19s 4ms/step - loss: 1.8596 - accuracy: 0.5709\n",
            "Epoch 171/200\n",
            "5040/5040 [==============================] - 18s 3ms/step - loss: 1.8843 - accuracy: 0.5653\n",
            "Epoch 172/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.8743 - accuracy: 0.5674\n",
            "Epoch 173/200\n",
            "5040/5040 [==============================] - 19s 4ms/step - loss: 1.8774 - accuracy: 0.5678\n",
            "Epoch 174/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.8676 - accuracy: 0.5691\n",
            "Epoch 175/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.8700 - accuracy: 0.5687\n",
            "Epoch 176/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.8728 - accuracy: 0.5680\n",
            "Epoch 177/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 1.8753 - accuracy: 0.5673\n",
            "Epoch 178/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 1.8959 - accuracy: 0.5632\n",
            "Epoch 179/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.8954 - accuracy: 0.5633\n",
            "Epoch 180/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.8877 - accuracy: 0.5642\n",
            "Epoch 181/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.8842 - accuracy: 0.5656\n",
            "Epoch 182/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 1.8760 - accuracy: 0.5666\n",
            "Epoch 183/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 1.8839 - accuracy: 0.5658\n",
            "Epoch 184/200\n",
            "5040/5040 [==============================] - 18s 4ms/step - loss: 1.8841 - accuracy: 0.5655\n",
            "Epoch 185/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.8960 - accuracy: 0.5625\n",
            "Epoch 186/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.9034 - accuracy: 0.5613\n",
            "Epoch 187/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.8917 - accuracy: 0.5638\n",
            "Epoch 188/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.9034 - accuracy: 0.5603\n",
            "Epoch 189/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.9041 - accuracy: 0.5609\n",
            "Epoch 190/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.9039 - accuracy: 0.5608\n",
            "Epoch 191/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.9147 - accuracy: 0.5578\n",
            "Epoch 192/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.9163 - accuracy: 0.5579\n",
            "Epoch 193/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.9156 - accuracy: 0.5585\n",
            "Epoch 194/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.9109 - accuracy: 0.5593\n",
            "Epoch 195/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.9094 - accuracy: 0.5596\n",
            "Epoch 196/200\n",
            "5040/5040 [==============================] - 19s 4ms/step - loss: 1.9159 - accuracy: 0.5590\n",
            "Epoch 197/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.9166 - accuracy: 0.5583\n",
            "Epoch 198/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.9060 - accuracy: 0.5608\n",
            "Epoch 199/200\n",
            "5040/5040 [==============================] - 16s 3ms/step - loss: 1.9265 - accuracy: 0.5554\n",
            "Epoch 200/200\n",
            "5040/5040 [==============================] - 17s 3ms/step - loss: 1.9177 - accuracy: 0.5583\n",
            "Model created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFCDNjHZl9DV",
        "outputId": "4a032bfd-6a54-4f86-bebd-22df837dde80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               16256     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 55)                3575      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 28087 (109.71 KB)\n",
            "Trainable params: 28087 (109.71 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tuz7GzV00rZ9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}